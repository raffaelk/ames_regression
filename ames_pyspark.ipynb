{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "careful-block",
   "metadata": {},
   "source": [
    "# House Price Prediction with PySpark\n",
    "\n",
    "Here, the hous price prediction task is solved by using the big data framework Apache Spark instead of scikit-learn. The house price data set is rather small, so using spark for this task is not efficient at all. But this model scales much better to bigger data sets, than a model using scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "registered-diameter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import log, exp\n",
    "from pyspark.ml.feature import Imputer, VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "import pyspark.ml.tuning as tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "incorrect-european",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relevant-theta",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "superior-introduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get train and test data set\n",
    "data_loc = './data'\n",
    "\n",
    "train_data_base = spark.read.csv(os.path.join(data_loc,'train.csv'), inferSchema=True, header=True, nullValue='NA')\n",
    "test_data_base = spark.read.csv(os.path.join(data_loc,'test.csv'), inferSchema=True, header=True, nullValue='NA')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designing-connection",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "focal-glossary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all numerical columns\n",
    "num_features = [col_name for col_name, dtype in train_data_base.dtypes if dtype == \"int\"]\n",
    "\n",
    "# remove SalePrice, which is the target, and the Id column from the list of features\n",
    "num_features.remove(\"SalePrice\")\n",
    "num_features.remove(\"Id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "restricted-radiation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split validation data\n",
    "train, val = train_data_base.randomSplit([.7, .3])\n",
    "\n",
    "# copy the test set\n",
    "test = test_data_base.select(\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "potential-fiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast all numerical features to double (necessary for imputation)\n",
    "for feat in num_features:\n",
    "    train = train.withColumn(feat, train[feat].cast(DoubleType()))\n",
    "    val = val.withColumn(feat, val[feat].cast(DoubleType()))\n",
    "    test = test.withColumn(feat, test[feat].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "responsible-bookmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log-scale the SalePrice\n",
    "train = train.withColumn(\"SalePriceLog\", log(\"SalePrice\"))\n",
    "val = val.withColumn(\"SalePriceLog\", log(\"SalePrice\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "damaged-liver",
   "metadata": {},
   "source": [
    "## ML Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "japanese-nudist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# names of the features afer imputation\n",
    "num_features_imp = [feat+\"_imp\" for feat in num_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "requested-adobe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the ML pipeline\n",
    "\n",
    "# imputation of missing values\n",
    "imputer = Imputer(inputCols=num_features, outputCols=num_features_imp)\n",
    "\n",
    "# assembler to collect all the features\n",
    "vec_assembler = VectorAssembler(inputCols=num_features_imp, outputCol=\"features\")\n",
    "\n",
    "# scaling of the fetures\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\", withStd=True, withMean=True)\n",
    "\n",
    "# ridge regression\n",
    "regression = LinearRegression(featuresCol=\"features_scaled\", labelCol=\"SalePriceLog\", elasticNetParam=0)\n",
    "\n",
    "# pipeline, combining all the steps\n",
    "pipe = Pipeline(stages=[imputer, vec_assembler, scaler, regression])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "southwest-placement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a parameter gird for hyperparameter tuning\n",
    "grid = tune.ParamGridBuilder()\n",
    "grid = grid.addGrid(regression.regParam, [0.001, 0.01, 0.1, 1, 10, 50, 100, 500, 1000])\n",
    "grid = grid.addGrid(imputer.strategy, [\"mean\", \"median\"])\n",
    "grid = grid.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "norman-orchestra",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the evaluation criteria\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"SalePriceLog\", metricName='rmse')\n",
    "\n",
    "# create the CrossValidator\n",
    "cv = tune.CrossValidator(estimator=pipe, estimatorParamMaps=grid, evaluator=evaluator)\n",
    "\n",
    "# fit cross validation models\n",
    "cv_models = cv.fit(train)\n",
    "\n",
    "# extract the best model\n",
    "bestPipeline = cv_models.bestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refined-better",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "worldwide-threshold",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19154988085065985"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict and evaluate the validation set\n",
    "val_prediction = bestPipeline.transform(val)\n",
    "evaluator.evaluate(val_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "hispanic-classification",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the test set\n",
    "test_prediction = bestPipeline.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "running-baking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the submission file \n",
    "submission = test_prediction.select(\"Id\", \"prediction\")\n",
    "submission = submission.withColumn(\"SalePrice\", exp(\"prediction\"))\n",
    "submission = submission.drop(\"prediction\")\n",
    "submission.write.csv(os.path.join(data_loc, \"submission_spark\"), header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "duplicate-democrat",
   "metadata": {},
   "source": [
    "The submission scored 0.14608 on the kaggle public leader board."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
